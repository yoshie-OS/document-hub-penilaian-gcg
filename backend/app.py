#!/usr/bin/env python3
"""
POS Data Cleaner 2 - Web API Backend
Integrates the production-ready processing engine (98.9% accuracy) with the web interface
"""

import os
import sys
import json
import uuid
import shutil
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import storage service
from storage_service import storage_service

# Project root for subprocess calls to the working core system
project_root = str(Path(__file__).parent.parent.parent)

app = Flask(__name__)
CORS(app)  # Enable CORS for React frontend

# Configuration
UPLOAD_FOLDER = Path(__file__).parent / 'uploads'
OUTPUT_FOLDER = Path(__file__).parent / 'outputs'
ALLOWED_EXTENSIONS = {'xlsx', 'xls', 'pdf', 'png', 'jpg', 'jpeg'}

# Ensure directories exist
UPLOAD_FOLDER.mkdir(exist_ok=True)
OUTPUT_FOLDER.mkdir(exist_ok=True)

app.config['UPLOAD_FOLDER'] = str(UPLOAD_FOLDER)
app.config['OUTPUT_FOLDER'] = str(OUTPUT_FOLDER)
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

def allowed_file(filename: str) -> bool:
    """Check if file extension is allowed."""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_file_type(filename: str) -> str:
    """Determine file type from extension."""
    ext = filename.rsplit('.', 1)[1].lower()
    if ext in {'xlsx', 'xls'}:
        return 'excel'
    elif ext == 'pdf':
        return 'pdf'
    elif ext in {'png', 'jpg', 'jpeg'}:
        return 'image'
    return 'unknown'

@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint."""
    return jsonify({
        'status': 'healthy',
        'service': 'POS Data Cleaner 2 API',
        'version': '2.0.0',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """
    Upload and process GCG assessment document.
    
    Expected form data:
    - file: The document file
    - checklistId: (optional) Associated checklist item ID
    - year: (optional) Assessment year
    - aspect: (optional) GCG aspect
    """
    try:
        print(f"ðŸ”§ DEBUG: Upload request received")
        print(f"ðŸ”§ DEBUG: Request files: {list(request.files.keys())}")
        
        # Check if file is present
        if 'file' not in request.files:
            print(f"ðŸ”§ DEBUG: No file in request")
            return jsonify({'error': 'No file provided'}), 400
        
        file = request.files['file']
        print(f"ðŸ”§ DEBUG: File received: {file.filename}")
        
        if file.filename == '':
            print(f"ðŸ”§ DEBUG: Empty filename")
            return jsonify({'error': 'No file selected'}), 400
        
        if not allowed_file(file.filename):
            print(f"ðŸ”§ DEBUG: File type not allowed: {file.filename}")
            return jsonify({'error': 'File type not allowed'}), 400
        
        print(f"ðŸ”§ DEBUG: File validation passed")
        
        try:
            print(f"ðŸ”§ DEBUG: Starting file processing...")
            # Generate unique filename
            file_id = str(uuid.uuid4())
            print(f"ðŸ”§ DEBUG: Generated file_id: {file_id}")
            original_filename = secure_filename(file.filename)
            filename_parts = original_filename.rsplit('.', 1)
            unique_filename = f"{file_id}_{filename_parts[0]}.{filename_parts[1]}"
            
            # Save uploaded file
            input_path = UPLOAD_FOLDER / unique_filename
            file.save(str(input_path))
            
            # Generate output filename
            output_filename = f"processed_{file_id}_{filename_parts[0]}.xlsx"
            output_path = OUTPUT_FOLDER / output_filename
            
            # Get metadata from form
            checklist_id = request.form.get('checklistId')
            year = request.form.get('year')
            aspect = request.form.get('aspect')
            
            # Process the document using production system
            file_type = get_file_type(original_filename)
            
            if file_type == 'excel':
                print(f"ðŸ”§ DEBUG: Processing Excel file using core system (subprocess)...")
                processing_result = None  # Force use of subprocess method
                
                # DISABLED: Accurate processing has pandas column selection issue
                # Fall through to subprocess method which works perfectly
            
            # Use subprocess method for all file types (Excel, PDF, Image)
            if file_type in ['excel', 'pdf', 'image']:
                print(f"ðŸ”§ DEBUG: Processing {file_type} file using core system...")
                
                try:
                    import time
                    start_time = time.time()
                    
                    # Call the working core system directly as subprocess
                    cmd = [
                        sys.executable, "main_new.py",
                        "-i", str(input_path),
                        "-o", str(output_path),
                        "-v"
                    ]
                    
                    print(f"ðŸ”§ DEBUG: Running command: {' '.join(cmd)}")
                    print(f"ðŸ”§ DEBUG: Working directory: {project_root}")
                    
                    result = subprocess.run(
                        cmd,
                        cwd=project_root,
                        capture_output=True,
                        text=True,
                        timeout=180  # 3 minute timeout for OCR processing
                    )
                    
                    end_time = time.time()
                    print(f"ðŸ”§ DEBUG: Core system completed in {end_time - start_time:.2f} seconds")
                    print(f"ðŸ”§ DEBUG: Return code: {result.returncode}")
                    print(f"ðŸ”§ DEBUG: STDOUT: {result.stdout}")
                    if result.stderr:
                        print(f"ðŸ”§ DEBUG: STDERR: {result.stderr}")
                    
                    if result.returncode == 0:
                        processing_result = {
                            'success': True,
                            'method': f'{file_type}_processing',
                            'message': 'Processing completed successfully',
                            'stdout': result.stdout,
                            'processing_time': f"{end_time - start_time:.2f}s"
                        }
                    else:
                        processing_result = {
                            'success': False,
                            'method': f'{file_type}_processing',
                            'error': f'Core system failed with code {result.returncode}',
                            'stdout': result.stdout,
                            'stderr': result.stderr
                        }
                    
                except subprocess.TimeoutExpired:
                    processing_result = {
                        'success': False,
                        'method': f'{file_type}_processing',
                        'error': 'Processing timeout (3 minutes exceeded)'
                    }
                except Exception as e:
                    print(f"ðŸ”§ DEBUG: EXCEPTION in subprocess call: {e}")
                    import traceback
                    print(f"ðŸ”§ DEBUG: Full traceback: {traceback.format_exc()}")
                    processing_result = {
                        'success': False,
                        'method': f'{file_type}_processing',
                        'error': f'Subprocess failed: {str(e)}'
                    }
            
            else:
                processing_result = {
                    'success': False,
                    'error': f'Unsupported file type: {file_type}',
                    'method': 'unsupported'
                }
        
        except Exception as proc_error:
            processing_result = {
                'success': False,
                'error': f'Processing failed: {str(proc_error)}',
                'method': 'processing_error'
            }
        
        # Load processed results if successful
        extracted_data = None
        if processing_result['success'] and output_path.exists():
            try:
                # Read the processed Excel file
                df = pd.read_excel(str(output_path))
                print(f"ðŸ”§ DEBUG: Loaded DataFrame with {len(df)} rows")
                print(f"ðŸ”§ DEBUG: DataFrame columns: {list(df.columns)}")
                print(f"ðŸ”§ DEBUG: DataFrame head:\n{df.head()}")
                
                # Extract key metrics
                indicator_rows = df[df['Type'] == 'indicator'] if 'Type' in df.columns else df
                subtotal_rows = df[df['Type'] == 'subtotal'] if 'Type' in df.columns else pd.DataFrame()
                total_rows = df[df['Type'] == 'total'] if 'Type' in df.columns else pd.DataFrame()
                print(f"ðŸ”§ DEBUG: Found {len(indicator_rows)} indicator rows")
                
                extracted_data = {
                    'total_rows': int(len(df)),
                    'indicators': int(len(indicator_rows)),
                    'subtotals': int(len(subtotal_rows)),
                    'totals': int(len(total_rows)),
                    'year': str(df['Tahun'].iloc[0]) if len(df) > 0 and pd.notna(df['Tahun'].iloc[0]) else None,
                    'penilai': str(df['Penilai'].iloc[0]) if len(df) > 0 and pd.notna(df['Penilai'].iloc[0]) else None,
                    'format_type': 'DETAILED' if len(df) > 20 else 'BRIEF',
                    'processing_status': 'success'
                }
                
                # Extract ALL indicator data (not just samples)
                if len(indicator_rows) > 0:
                    all_indicators = []
                    for _, row in indicator_rows.iterrows():
                        all_indicators.append({
                            'no': int(row['No']) if pd.notna(row['No']) else 0,
                            'section': str(row['Section']) if pd.notna(row['Section']) else '',
                            'description': str(row['Deskripsi']) if pd.notna(row['Deskripsi']) else '',
                            'jumlah_parameter': int(row['Jumlah_Parameter']) if pd.notna(row['Jumlah_Parameter']) else 0,
                            'bobot': float(row['Bobot']) if pd.notna(row['Bobot']) else 100.0,
                            'skor': float(row['Skor']) if pd.notna(row['Skor']) else 0.0,
                            'capaian': float(row['Capaian']) if pd.notna(row['Capaian']) else 0.0,
                            'penjelasan': str(row['Penjelasan']) if pd.notna(row['Penjelasan']) else 'Sangat Kurang'
                        })
                    extracted_data['sample_indicators'] = all_indicators
                    
                # Add sheet analysis for XLSX files and extract BRIEF data for aspect summary
                if file_type == 'excel':
                    try:
                        # Read Excel file to analyze sheets
                        excel_file = pd.ExcelFile(str(input_path))
                        sheet_names = excel_file.sheet_names
                        
                        sheet_analysis = {
                            'total_sheets': len(sheet_names),
                            'sheet_names': sheet_names,
                            'sheet_types': {}
                        }
                        
                        brief_sheet_data = None
                        
                        # Analyze each sheet to determine if it's BRIEF or DETAILED
                        for sheet_name in sheet_names:
                            try:
                                sheet_df = pd.read_excel(str(input_path), sheet_name=sheet_name)
                                
                                # Debug: Print sheet info
                                print(f"ðŸ”§ DEBUG: Analyzing sheet '{sheet_name}' with {len(sheet_df)} rows")
                                print(f"ðŸ”§ DEBUG: Sheet columns: {list(sheet_df.columns)}")
                                print(f"ðŸ”§ DEBUG: First few rows:\n{sheet_df.head()}")
                                
                                # Simple heuristic: BRIEF has fewer rows, DETAILED has more
                                if len(sheet_df) <= 15:
                                    sheet_type = 'BRIEF'
                                    
                                    # Try to extract BRIEF data from any sheet with reasonable data
                                    if len(sheet_df) >= 3 and len(sheet_df) <= 20:  # More flexible range
                                        brief_sheet_data = []
                                        
                                        print(f"ðŸ”§ DEBUG: Attempting BRIEF extraction from sheet '{sheet_name}'")
                                        
                                        for idx, row in sheet_df.iterrows():
                                            # Extract BRIEF data for aspect summary
                                            brief_row = {}
                                            
                                            # More flexible column matching
                                            for col in sheet_df.columns:
                                                col_str = str(col).strip()
                                                col_lower = col_str.lower()
                                                
                                                # Match various column patterns
                                                if any(keyword in col_lower for keyword in ['aspek', 'section', 'aspect']):
                                                    brief_row['aspek'] = str(row[col]).strip() if pd.notna(row[col]) else ''
                                                elif any(keyword in col_lower for keyword in ['deskripsi', 'description', 'desc']):
                                                    brief_row['deskripsi'] = str(row[col]).strip() if pd.notna(row[col]) else ''
                                                elif any(keyword in col_lower for keyword in ['bobot', 'weight', 'berat']):
                                                    try:
                                                        brief_row['bobot'] = float(row[col]) if pd.notna(row[col]) else 0.0
                                                    except (ValueError, TypeError):
                                                        brief_row['bobot'] = 0.0
                                                elif any(keyword in col_lower for keyword in ['skor', 'score', 'nilai']):
                                                    try:
                                                        brief_row['skor'] = float(row[col]) if pd.notna(row[col]) else 0.0
                                                    except (ValueError, TypeError):
                                                        brief_row['skor'] = 0.0
                                                elif any(keyword in col_lower for keyword in ['capaian', 'achievement', 'pencapaian']):
                                                    try:
                                                        brief_row['capaian'] = float(row[col]) if pd.notna(row[col]) else 0.0
                                                    except (ValueError, TypeError):
                                                        brief_row['capaian'] = 0.0
                                                elif any(keyword in col_lower for keyword in ['penjelasan', 'explanation', 'keterangan']):
                                                    brief_row['penjelasan'] = str(row[col]).strip() if pd.notna(row[col]) else ''
                                            
                                            # Debug: show what we extracted for this row
                                            print(f"ðŸ”§ DEBUG: Row {idx}: {brief_row}")
                                            
                                            # Add row if it has meaningful data (aspek is required)
                                            if brief_row.get('aspek') and brief_row.get('aspek').strip() and brief_row.get('aspek') != 'nan':
                                                brief_sheet_data.append(brief_row)
                                        
                                        print(f"ðŸ”§ DEBUG: Successfully extracted {len(brief_sheet_data)} BRIEF summary rows from sheet '{sheet_name}'")
                                        
                                else:
                                    sheet_type = 'DETAILED'
                                    
                                sheet_analysis['sheet_types'][sheet_name] = {
                                    'type': sheet_type,
                                    'row_count': len(sheet_df),
                                    'contains_summary_data': len(sheet_df) <= 10 and len(sheet_df) >= 5
                                }
                            except Exception as e:
                                sheet_analysis['sheet_types'][sheet_name] = {
                                    'type': 'UNKNOWN',
                                    'error': str(e)
                                }
                        
                        extracted_data['sheet_analysis'] = sheet_analysis
                        extracted_data['brief_sheet_data'] = brief_sheet_data
                        
                    except Exception as e:
                        extracted_data['sheet_analysis'] = {
                            'error': f'Could not analyze sheets: {str(e)}'
                        }
                
            except Exception as read_error:
                extracted_data = {
                    'error': f'Could not read processed file: {str(read_error)}'
                }
        
        # Prepare response
        response_data = {
            'fileId': file_id,
            'originalFilename': original_filename,
            'processedFilename': output_filename,
            'fileType': file_type,
            'fileSize': input_path.stat().st_size,
            'uploadTime': datetime.now().isoformat(),
            'processing': processing_result,
            'extractedData': extracted_data,
            'metadata': {
                'checklistId': checklist_id,
                'year': year,
                'aspect': aspect
            }
        }
        
        return jsonify(response_data), 200
        
    except Exception as e:
        print(f"ðŸ”§ DEBUG: Exception occurred: {str(e)}")
        import traceback
        print(f"ðŸ”§ DEBUG: Full traceback: {traceback.format_exc()}")
        return jsonify({'error': f'Upload failed: {str(e)}'}), 500

@app.route('/api/download/<file_id>', methods=['GET'])
def download_file(file_id: str):
    """Download processed file by ID."""
    try:
        # Find the processed file
        for output_file in OUTPUT_FOLDER.glob(f"processed_{file_id}_*.xlsx"):
            if output_file.exists():
                return send_file(
                    str(output_file),
                    as_attachment=True,
                    download_name=f"GCG_Assessment_{file_id}.xlsx",
                    mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                )
        
        return jsonify({'error': 'File not found'}), 404
        
    except Exception as e:
        return jsonify({'error': f'Download failed: {str(e)}'}), 500

@app.route('/api/files', methods=['GET'])
def list_files():
    """List all processed files."""
    try:
        files = []
        
        for output_file in OUTPUT_FOLDER.glob("processed_*.xlsx"):
            # Extract file ID from filename
            filename_parts = output_file.name.split('_', 2)
            if len(filename_parts) >= 2:
                file_id = filename_parts[1]
                
                # Get file stats
                stat = output_file.stat()
                
                files.append({
                    'fileId': file_id,
                    'filename': output_file.name,
                    'size': stat.st_size,
                    'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
                })
        
        return jsonify({'files': files}), 200
        
    except Exception as e:
        return jsonify({'error': f'Failed to list files: {str(e)}'}), 500

@app.route('/api/system/info', methods=['GET'])
def system_info():
    """Get system information and capabilities."""
    return jsonify({
        'system': 'POS Data Cleaner 2',
        'version': '2.0.0',
        'capabilities': {
            'file_types': list(ALLOWED_EXTENSIONS),
            'formats_supported': ['DETAILED (56 rows)', 'BRIEF (13 rows)'],
            'languages': ['Indonesian'],
            'years_supported': '2014-2025',
            'gcg_aspects': ['I-VI (Roman numerals)', 'A-H (Alphabetic)', '1-10 (Numeric)'],
            'advanced_features': [
                'Mathematical topology processing',
                'Quantum superposition layouts', 
                'DNA helix patterns',
                'Fractal recursive structures',
                'Multi-engine OCR (Tesseract + PaddleOCR)',
                'ML classification (XGBoost + rules)'
            ]
        },
        'processing_pipeline': [
            'File type detection',
            'Format classification (DETAILED vs BRIEF)',
            'Pattern recognition (43+ indicator patterns)', 
            'Spatial matching (distance-based pairing)',
            'Manual.xlsx structure generation (362 rows)',
            'Quality validation'
        ],
        'infrastructure': {
            'privacy_first': True,
            'cloud_dependencies': None,
            'local_processing': True,
            'max_file_size': '16MB',
            'concurrent_processing': True
        }
    })


@app.route('/api/save', methods=['POST'])
def save_assessment():
    """
    Save assessment data directly to output.xlsx (no JSON intermediate)
    """
    try:
        data = request.json
        print(f"ðŸ”§ DEBUG: Received save request with data keys: {data.keys()}")
        
        # Create assessment record
        assessment_id = f"{data.get('year', 'unknown')}_{data.get('auditor', 'unknown')}_{str(uuid.uuid4())[:8]}"
        saved_at = datetime.now().isoformat()
        
        # Load existing XLSX data and COMPLETELY REPLACE year's data (including deletions)
        all_rows = []
        existing_df = storage_service.read_excel('web-output/output.xlsx')
        
        if existing_df is not None:
            try:
                current_year = data.get('year')
                
                print(f"ðŸ”§ DEBUG: Loading existing XLSX with {len(existing_df)} rows")
                print(f"ðŸ”§ DEBUG: Current year to save: {current_year}")
                print(f"ðŸ”§ DEBUG: Existing years in file: {existing_df['Tahun'].unique().tolist()}")
                
                # COMPLETELY REMOVE all existing data for this year (this handles deletions)
                if current_year:
                    original_count = len(existing_df)
                    existing_df = existing_df[existing_df['Tahun'] != current_year]
                    removed_count = original_count - len(existing_df)
                    print(f"ðŸ”§ DEBUG: COMPLETELY REMOVED {removed_count} rows for year {current_year} (including deletions)")
                    print(f"ðŸ”§ DEBUG: Preserved {len(existing_df)} rows from other years")
                
                # Convert remaining data back to list format
                for _, row in existing_df.iterrows():
                    all_rows.append(row.to_dict())
                    
                print(f"ðŸ”§ DEBUG: Starting with {len(all_rows)} rows from other years")
            except Exception as e:
                print(f"WARNING: Could not read existing XLSX: {e}")
        
        # Process new data and add to all_rows
        year = data.get('year', 'unknown')
        auditor = data.get('auditor', 'unknown')
        jenis_asesmen = data.get('jenis_asesmen', 'Internal')
        
        # Process main indicator data
        for row in data.get('data', []):
            # Map frontend data structure to XLSX format
            row_id = row.get('id', row.get('no', ''))
            section = row.get('aspek', row.get('section', ''))
            is_total = row.get('isTotal', False)
            
            # Determine Level and Type based on data structure
            if is_total:
                level = "1"
                row_type = "total"
            elif str(row_id).isdigit():
                level = "2"
                row_type = "indicator"
            else:
                level = "1"
                row_type = "header"
            
            xlsx_row = {
                'Level': level,
                'Type': row_type,
                'Section': section,
                'No': row_id,
                'Deskripsi': row.get('deskripsi', ''),
                'Bobot': row.get('bobot', ''),
                'Skor': row.get('skor', ''),
                'Capaian': row.get('capaian', ''),
                'Penjelasan': row.get('penjelasan', ''),
                'Tahun': year,
                'Penilai': auditor,
                'Jenis_Asesmen': jenis_asesmen,
                'Export_Date': saved_at[:10]
            }
            all_rows.append(xlsx_row)
        
        # Process aspect summary data (if provided)
        aspect_summary_data = data.get('aspectSummaryData', [])
        if aspect_summary_data:
            print(f"ðŸ”§ DEBUG: Processing {len(aspect_summary_data)} aspect summary rows")
            
            for summary_row in aspect_summary_data:
                section = summary_row.get('aspek', '')
                deskripsi = summary_row.get('deskripsi', '')
                bobot = summary_row.get('bobot', 0)
                skor = summary_row.get('skor', 0)
                
                # Skip empty aspects or meaningless default data
                if not section or not deskripsi or (bobot == 0 and skor == 0):
                    continue
                    
                # Skip if this looks like an unedited default row (just roman numerals with no real data)
                if section in ['I', 'II', 'III', 'IV', 'V', 'VI'] and not deskripsi.strip():
                    continue
                    
                # Row 1: Header for this aspect
                header_row = {
                    'Level': "1",
                    'Type': 'header',
                    'Section': section,
                    'No': '',
                    'Deskripsi': summary_row.get('deskripsi', ''),
                    'Bobot': '',
                    'Skor': '',
                    'Capaian': '',
                    'Penjelasan': '',
                    'Tahun': year,
                    'Penilai': auditor,
                    'Jenis_Asesmen': jenis_asesmen,
                    'Export_Date': saved_at[:10]
                }
                all_rows.append(header_row)
                
                # Row 2: Subtotal for this aspect
                subtotal_row = {
                    'Level': "1", 
                    'Type': 'subtotal',
                    'Section': section,
                    'No': '',
                    'Deskripsi': f'JUMLAH {section}',
                    'Bobot': summary_row.get('bobot', ''),
                    'Skor': summary_row.get('skor', ''),
                    'Capaian': summary_row.get('capaian', ''),
                    'Penjelasan': summary_row.get('penjelasan', ''),
                    'Tahun': year,
                    'Penilai': auditor,
                    'Jenis_Asesmen': jenis_asesmen,
                    'Export_Date': saved_at[:10]
                }
                all_rows.append(subtotal_row)
        
        # Process separate totalData (total row sent separately from main data)
        total_data = data.get('totalData', {})
        if total_data and isinstance(total_data, dict):
            # Check if total data has meaningful values (not all zeros)
            has_meaningful_total = (
                total_data.get('bobot', 0) != 0 or 
                total_data.get('skor', 0) != 0 or 
                total_data.get('capaian', 0) != 0 or 
                total_data.get('penjelasan', '').strip() != ''
            )
            
            if has_meaningful_total:
                print(f"ðŸ”§ DEBUG: Processing separate totalData: {total_data}")
                
                total_row = {
                    'Level': "4",
                    'Type': 'total',
                    'Section': 'TOTAL',
                    'No': '',
                    'Deskripsi': 'TOTAL',
                    'Bobot': total_data.get('bobot', ''),
                    'Skor': total_data.get('skor', ''),
                    'Capaian': total_data.get('capaian', ''),
                    'Penjelasan': total_data.get('penjelasan', ''),
                    'Tahun': year,
                    'Penilai': auditor,
                    'Jenis_Asesmen': jenis_asesmen,
                    'Export_Date': saved_at[:10]
                }
                all_rows.append(total_row)
                print(f"ðŸ”§ DEBUG: Added totalData row to all_rows")
            else:
                print(f"ðŸ”§ DEBUG: Skipping totalData - no meaningful values")
        
        # Convert to DataFrame and save XLSX
        if all_rows:
            df = pd.DataFrame(all_rows)
            
            # Remove any duplicate rows
            df_unique = df.drop_duplicates(subset=['Tahun', 'Section', 'No', 'Deskripsi'], keep='last')
            print(f"ðŸ”§ DEBUG: Removed {len(df) - len(df_unique)} duplicate rows")
            
            # Custom sorting: year â†’ aspek â†’ no, then organize headers and subtotals properly
            def sort_key(row):
                # Ensure all values are consistently typed for comparison
                try:
                    year = int(row['Tahun']) if pd.notna(row['Tahun']) else 0
                except (ValueError, TypeError):
                    year = 0
                
                section = str(row['Section']) if pd.notna(row['Section']) else ''
                no = row['No']
                row_type = str(row['Type']) if pd.notna(row['Type']) else 'indicator'
                
                # Convert 'no' to numeric for proper sorting, handle empty values
                try:
                    no_numeric = int(no) if str(no).isdigit() else 9999
                except (ValueError, TypeError):
                    no_numeric = 9999
                
                # Type priority: header=0, indicators=1, subtotal=2, total=3 (appears last)
                type_priority = {'header': 0, 'indicator': 1, 'subtotal': 2, 'total': 3}.get(row_type, 1)
                
                # Special handling for total rows: they should appear at the very end of each year
                if row_type == 'total':
                    # Use 'ZZZZZ' as section to ensure total rows sort last within each year
                    section = 'ZZZZZ'
                
                return (year, section, type_priority, no_numeric)
            
            # Apply custom sorting
            df_sorted = df_unique.loc[df_unique.apply(sort_key, axis=1).sort_values().index]
            
            # Save XLSX using storage service
            success = storage_service.write_excel(df_sorted, 'web-output/output.xlsx')
            if success:
                print(f"SUCCESS: Saved to output.xlsx with {len(df_sorted)} rows (sorted: year->aspek->no->type)")
            else:
                print(f"ERROR: Failed to save output.xlsx")
            
        return jsonify({
            'success': True,
            'message': 'Data berhasil disimpan',
            'assessment_id': assessment_id,
            'saved_at': saved_at
        })
        
    except Exception as e:
        print(f"ERROR: Error saving assessment: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# generate_output_xlsx function removed - now saving directly to XLSX


@app.route('/api/delete-year-data', methods=['DELETE'])
def delete_year_data():
    """
    Delete all assessment data for a specific year from output.xlsx
    """
    try:
        data = request.json
        year_to_delete = data.get('year')
        
        if not year_to_delete:
            return jsonify({
                'success': False,
                'error': 'Year parameter is required'
            }), 400
        
        print(f"ðŸ—‘ï¸ DEBUG: Received delete request for year: {year_to_delete}")
        
        # Load existing XLSX data
        existing_df = storage_service.read_excel('web-output/output.xlsx')
        
        if existing_df is None:
            return jsonify({
                'success': False,
                'error': 'No data file exists to delete from'
            }), 404
        
        try:
            print(f"ðŸ”§ DEBUG: Loading existing XLSX with {len(existing_df)} rows")
            print(f"ðŸ”§ DEBUG: Year to delete: {year_to_delete}")
            print(f"ðŸ”§ DEBUG: Existing years in file: {existing_df['Tahun'].unique().tolist()}")
            
            # Check if the year exists in the data
            if year_to_delete not in existing_df['Tahun'].values:
                return jsonify({
                    'success': False,
                    'error': f'No data found for year {year_to_delete}'
                }), 404
            
            # Remove all data for the specified year
            original_count = len(existing_df)
            filtered_df = existing_df[existing_df['Tahun'] != year_to_delete]
            deleted_count = original_count - len(filtered_df)
            
            print(f"ðŸ—‘ï¸ DEBUG: Deleted {deleted_count} rows for year {year_to_delete}")
            print(f"ðŸ”§ DEBUG: Remaining {len(filtered_df)} rows from other years")
            
            # Save the filtered data back to the XLSX file
            if len(filtered_df) > 0:
                # Sort the remaining data properly before saving
                def sort_key(row):
                    year = row['Tahun']
                    section = str(row['Section']) if pd.notna(row['Section']) else ''
                    no = row['No']
                    row_type = str(row['Type']) if pd.notna(row['Type']) else 'indicator'
                    
                    # Convert 'no' to numeric for proper sorting
                    try:
                        no_numeric = int(no) if str(no).isdigit() else 9999
                    except (ValueError, TypeError):
                        no_numeric = 9999
                    
                    # Type priority: header=0, indicators=1, subtotal=2, total=3 (appears last)
                    type_priority = {'header': 0, 'indicator': 1, 'subtotal': 2, 'total': 3}.get(row_type, 1)
                    
                    # Special handling for total rows: they should appear at the very end of each year
                    if row_type == 'total':
                        # Use 'ZZZZZ' as section to ensure total rows sort last within each year
                        section = 'ZZZZZ'
                    
                    return (year, section, type_priority, no_numeric)
                
                # Apply sorting
                df_sorted = filtered_df.loc[filtered_df.apply(sort_key, axis=1).sort_values().index]
                success = storage_service.write_excel(df_sorted, 'web-output/output.xlsx')
                if success:
                    print(f"SUCCESS: Updated output.xlsx with {len(df_sorted)} rows (deleted {deleted_count} rows for year {year_to_delete})")
                else:
                    print(f"ERROR: Failed to update output.xlsx after deletion")
            else:
                # If no data remains, create an empty file with just headers
                empty_df = pd.DataFrame(columns=['Level', 'Type', 'Section', 'No', 'Deskripsi', 
                                               'Bobot', 'Skor', 'Capaian', 'Penjelasan', 'Tahun', 'Penilai', 
                                               'Jenis_Asesmen', 'Export_Date'])
                success = storage_service.write_excel(empty_df, 'web-output/output.xlsx')
                if success:
                    print(f"SUCCESS: Created empty output.xlsx file (all data deleted)")
                else:
                    print(f"ERROR: Failed to create empty output.xlsx file")
            
        except Exception as e:
            print(f"ERROR: Could not process XLSX file: {e}")
            return jsonify({
                'success': False,
                'error': f'Could not process XLSX file: {str(e)}'
            }), 500
        
        return jsonify({
            'success': True,
            'message': f'Data untuk tahun {year_to_delete} berhasil dihapus',
            'deleted_rows': deleted_count,
            'year': year_to_delete
        })
        
    except Exception as e:
        print(f"ERROR: Error deleting year data: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/load/<int:year>', methods=['GET'])
def load_assessment_by_year(year):
    """
    Load assessment data for a specific year from output.xlsx
    """
    try:
        # Read XLSX data
        df = storage_service.read_excel('web-output/output.xlsx')
        
        if df is None:
            return jsonify({
                'success': False,
                'data': [],
                'message': f'No saved data found for year {year}'
            })
        
        # Filter for the requested year
        year_df = df[df['Tahun'] == year]
        
        if len(year_df) > 0:
            print(f"ðŸ”§ DEBUG: Processing {len(year_df)} rows for year {year}")
            
            # Detect format: BRIEF or DETAILED based on data types
            indicator_rows = year_df[year_df['Type'] == 'indicator']
            subtotal_rows = year_df[year_df['Type'] == 'subtotal'] 
            header_rows = year_df[year_df['Type'] == 'header']
            
            is_detailed = len(indicator_rows) > 10 and len(subtotal_rows) > 0
            format_type = 'DETAILED' if is_detailed else 'BRIEF'
            
            print(f"ðŸ”§ DEBUG: Detected format: {format_type}")
            print(f"ðŸ”§ DEBUG: Found {len(indicator_rows)} indicators, {len(subtotal_rows)} subtotals, {len(header_rows)} headers")
            
            # Process indicator data for main table (both BRIEF and DETAILED)
            main_table_data = []
            for _, row in indicator_rows.iterrows():
                row_id = row.get('No', '')
                if pd.isna(row_id) or str(row_id).lower() in ['nan', '', 'none']:
                    continue
                    
                aspek = str(row.get('Section', ''))
                deskripsi = str(row.get('Deskripsi', ''))
                if not aspek or not deskripsi:
                    continue
                
                penjelasan = row.get('Penjelasan', '')
                if pd.isna(penjelasan) or str(penjelasan).lower() == 'nan':
                    penjelasan = 'Tidak Baik'
                
                main_table_data.append({
                    'id': str(row_id),
                    'aspek': aspek,
                    'deskripsi': deskripsi,
                    'jumlah_parameter': int(row.get('Jumlah_Parameter', 0)) if pd.notna(row.get('Jumlah_Parameter')) else 0,
                    'bobot': float(row.get('Bobot', 0)) if pd.notna(row.get('Bobot')) else 0,
                    'skor': float(row.get('Skor', 0)) if pd.notna(row.get('Skor')) else 0,
                    'capaian': float(row.get('Capaian', 0)) if pd.notna(row.get('Capaian')) else 0,
                    'penjelasan': str(penjelasan)
                })
            
            # Process aspek summary data (subtotals) for DETAILED mode
            aspek_summary_data = []
            if is_detailed and len(subtotal_rows) > 0:
                for _, row in subtotal_rows.iterrows():
                    aspek = str(row.get('Section', ''))
                    if not aspek:
                        continue
                        
                    penjelasan = row.get('Penjelasan', '')
                    if pd.isna(penjelasan) or str(penjelasan).lower() == 'nan':
                        penjelasan = 'Tidak Baik'
                    
                    aspek_summary_data.append({
                        'id': f'summary-{aspek}',
                        'aspek': aspek,
                        'deskripsi': str(row.get('Deskripsi', '')),
                        'jumlah_parameter': int(row.get('Jumlah_Parameter', 0)) if pd.notna(row.get('Jumlah_Parameter')) else 0,
                        'bobot': float(row.get('Bobot', 0)) if pd.notna(row.get('Bobot')) else 0,
                        'skor': float(row.get('Skor', 0)) if pd.notna(row.get('Skor')) else 0,
                        'capaian': float(row.get('Capaian', 0)) if pd.notna(row.get('Capaian')) else 0,
                        'penjelasan': str(penjelasan)
                    })
            
            print(f"ðŸ”§ DEBUG: Processed {len(main_table_data)} indicators, {len(aspek_summary_data)} aspect summaries")
            
            # Get auditor and jenis_asesmen from first row
            auditor = year_df.iloc[0].get('Penilai', 'Unknown') if len(year_df) > 0 else 'Unknown'
            jenis_asesmen = year_df.iloc[0].get('Jenis_Asesmen', 'Internal') if len(year_df) > 0 else 'Internal'
            
            return jsonify({
                'success': True,
                'data': main_table_data,
                'aspek_summary_data': aspek_summary_data,
                'format_type': format_type,
                'is_detailed': is_detailed,
                'auditor': auditor,
                'jenis_asesmen': jenis_asesmen,
                'method': 'xlsx_load',
                'saved_at': year_df.iloc[0].get('Export_Date', '') if len(year_df) > 0 else '',
                'message': f'Loaded {len(main_table_data)} indicators + {len(aspek_summary_data)} summaries for year {year} ({format_type} format)'
            })
        else:
            return jsonify({
                'success': False,
                'data': [],
                'message': f'No saved data found for year {year}'
            })
            
    except Exception as e:
        print(f"ERROR: Error loading year {year}: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'data': []
        }), 500


@app.route('/api/dashboard-data', methods=['GET'])
def get_dashboard_data():
    """
    Get all assessment data from output.xlsx for dashboard visualization
    """
    try:
        # Read XLSX data
        df = storage_service.read_excel('web-output/output.xlsx')
        
        if df is None:
            return jsonify({
                'success': False,
                'data': [],
                'message': 'No dashboard data available. Please save some assessments first.'
            })
        
        print(f"ðŸ”§ DEBUG: Dashboard loading {len(df)} rows from output.xlsx")
        print(f"ðŸ”§ DEBUG: Years in file: {df['Tahun'].unique().tolist()}")
        print(f"ðŸ”§ DEBUG: Sample rows: {df[['Tahun', 'Section', 'Skor']].head().to_dict('records')}")
        
        # Convert to dashboard format
        dashboard_data = []
        for _, row in df.iterrows():
            # Handle NaN values properly
            bobot = row.get('Bobot', 0)
            skor = row.get('Skor', 0)
            capaian = row.get('Capaian', 0)
            jumlah_param = row.get('Jumlah_Parameter', 0)
            
            # Convert NaN to 0 for numeric fields
            if pd.isna(bobot):
                bobot = 0
            if pd.isna(skor):
                skor = 0
            if pd.isna(capaian):
                capaian = 0
            if pd.isna(jumlah_param):
                jumlah_param = 0
                
            dashboard_item = {
                'id': str(row.get('No', '')),
                'aspek': str(row.get('Section', '')),
                'deskripsi': str(row.get('Deskripsi', '')),
                'jumlah_parameter': float(jumlah_param),
                'bobot': float(bobot),
                'skor': float(skor),
                'capaian': float(capaian),
                'penjelasan': str(row.get('Penjelasan', '')),
                'year': int(row.get('Tahun', 2022)),
                'auditor': str(row.get('Penilai', 'Unknown')),
                'jenis_asesmen': str(row.get('Jenis_Asesmen', 'Internal'))
            }
            dashboard_data.append(dashboard_item)
        
        # Group by year for multi-year support
        years_data = {}
        for item in dashboard_data:
            year = item['year']
            if year not in years_data:
                years_data[year] = {
                    'year': year,
                    'auditor': item['auditor'],
                    'jenis_asesmen': item['jenis_asesmen'],
                    'data': []
                }
            years_data[year]['data'].append(item)
        
        return jsonify({
            'success': True,
            'years_data': years_data,
            'total_rows': len(dashboard_data),
            'available_years': list(years_data.keys()),
            'message': f'Loaded dashboard data for {len(years_data)} year(s)'
        })
        
    except Exception as e:
        print(f"ERROR: Error loading dashboard data: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'data': []
        }), 500


@app.route('/api/aspek-data', methods=['GET'])
def get_aspek_data():
    """
    Get hybrid data (subtotal + header) for aspek summary table
    """
    try:
        # Read XLSX data
        df = storage_service.read_excel('web-output/output.xlsx')
        
        if df is None:
            return jsonify({
                'success': False,
                'data': [],
                'message': 'No data available'
            })
        
        # Create hybrid data: subtotal numeric data + header descriptions
        subtotal_rows = df[df['Type'] == 'subtotal']
        header_rows = df[df['Type'] == 'header']
        
        # Convert to frontend format by combining subtotal + header data
        indicators = []
        for _, subtotal_row in subtotal_rows.iterrows():
            # Find matching header row by Section and Year
            matching_header = header_rows[
                (header_rows['Section'] == subtotal_row['Section']) & 
                (header_rows['Tahun'] == subtotal_row['Tahun'])
            ]
            
            # Use header description if found, otherwise subtotal description
            deskripsi = subtotal_row['Deskripsi']  # fallback
            if not matching_header.empty:
                deskripsi = matching_header.iloc[0]['Deskripsi']
            indicators.append({
                'id': str(subtotal_row.get('No', '')),
                'aspek': str(subtotal_row.get('Section', '')),
                'deskripsi': deskripsi,  # Use header description
                'jumlah_parameter': int(subtotal_row.get('Jumlah_Parameter', 0)) if pd.notna(subtotal_row.get('Jumlah_Parameter')) else 0,
                'bobot': float(subtotal_row.get('Bobot', 0)) if pd.notna(subtotal_row.get('Bobot')) else 0,
                'skor': float(subtotal_row.get('Skor', 0)) if pd.notna(subtotal_row.get('Skor')) else 0,
                'capaian': float(subtotal_row.get('Capaian', 0)) if pd.notna(subtotal_row.get('Capaian')) else 0,
                'penjelasan': str(subtotal_row.get('Penjelasan', 'Tidak Baik')),
                'tahun': int(subtotal_row.get('Tahun', 0)) if pd.notna(subtotal_row.get('Tahun')) else 0
            })
        
        return jsonify({
            'success': True,
            'data': indicators,
            'total': len(indicators),
            'message': f'Loaded {len(indicators)} aspek records'
        })
        
    except Exception as e:
        print(f"ERROR: Error loading aspek data: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'data': []
        }), 500

def _cleanup_orphaned_data_internal():
    """
    Internal helper function to clean up orphaned data (without returning HTTP response)
    """
    assessments_path = Path(__file__).parent.parent / 'web-output' / 'assessments.json'
    
    # Get years that exist in output.xlsx
    xlsx_years = set()
    df = storage_service.read_excel('web-output/output.xlsx')
    if df is not None:
        xlsx_years = set(df['Tahun'].unique())
    
    # Clean up assessments.json
    orphaned_count = 0
    if assessments_path.exists():
        with open(assessments_path, 'r') as f:
            assessments_data = json.load(f)
        
        # Filter out orphaned entries
        cleaned_assessments = []
        for assessment in assessments_data.get('assessments', []):
            year = assessment.get('year')
            if year in xlsx_years:
                cleaned_assessments.append(assessment)
            else:
                orphaned_count += 1
        
        # Save cleaned data if any changes were made
        if orphaned_count > 0:
            assessments_data['assessments'] = cleaned_assessments
            with open(assessments_path, 'w') as f:
                json.dump(assessments_data, f, indent=2)
            print(f"ðŸ”„ Auto-cleaned {orphaned_count} orphaned entries")
    
    return orphaned_count

@app.route('/api/indicator-data', methods=['GET'])
def get_indicator_data():
    """
    Get pure indicator data for detailed bottom table
    """
    try:
        # Auto-cleanup orphaned data before proceeding
        try:
            _cleanup_orphaned_data_internal()
        except Exception as cleanup_error:
            print(f"WARNING: Auto-cleanup failed: {cleanup_error}")
        
        # Read XLSX data
        df = storage_service.read_excel('web-output/output.xlsx')
        
        if df is None:
            return jsonify({
                'success': False,
                'data': [],
                'message': 'No data available'
            })
        
        # Filter only indicator rows
        indicator_rows = df[df['Type'] == 'indicator']
        
        # Convert to frontend format
        indicators = []
        for _, row in indicator_rows.iterrows():
            indicators.append({
                'id': str(row.get('No', '')),
                'aspek': str(row.get('Section', '')),
                'deskripsi': str(row.get('Deskripsi', '')),
                'jumlah_parameter': int(row.get('Jumlah_Parameter', 0)) if pd.notna(row.get('Jumlah_Parameter')) else 0,
                'bobot': float(row.get('Bobot', 0)) if pd.notna(row.get('Bobot')) else 0,
                'skor': float(row.get('Skor', 0)) if pd.notna(row.get('Skor')) else 0,
                'capaian': float(row.get('Capaian', 0)) if pd.notna(row.get('Capaian')) else 0,
                'penjelasan': str(row.get('Penjelasan', 'Tidak Baik')),
                'tahun': int(row.get('Tahun', 0)) if pd.notna(row.get('Tahun')) else 0
            })
        
        return jsonify({
            'success': True,
            'data': indicators,
            'total': len(indicators),
            'message': f'Loaded {len(indicators)} indicator records'
        })
        
    except Exception as e:
        print(f"ERROR: Error loading indicator data: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'data': []
        }), 500

@app.route('/api/gcg-chart-data', methods=['GET'])
def get_gcg_chart_data():
    """
    Get assessment data formatted for GCGChart component (graphics-2 format)
    Returns data with Level hierarchy as expected by processGCGData function
    """
    try:
        # Read XLSX data
        df = storage_service.read_excel('web-output/output.xlsx')
        
        if df is None:
            print(f"WARNING: output.xlsx not found or empty")
            return jsonify({
                'success': True,
                'data': [],
                'message': 'No chart data available. Please save some assessments first.'
            })
        
        print(f"INFO: GCG Chart Data: Loading {len(df)} rows from output.xlsx")
        
        # Convert to graphics-2 GCGData format
        gcg_data = []
        for _, row in df.iterrows():
            # Determine level based on row type
            level = 3  # Default to section level
            row_type = str(row.get('Type', '')).lower()
            
            if row_type == 'total':
                level = 4
            elif row_type == 'header':
                level = 1
            elif row_type == 'indicator':
                level = 2
            elif row_type == 'subtotal':
                level = 3
            
            # Handle NaN values
            tahun = int(row.get('Tahun', 2022))
            skor = float(row.get('Skor', 0)) if not pd.isna(row.get('Skor', 0)) else 0
            capaian = float(row.get('Capaian', 0)) if not pd.isna(row.get('Capaian', 0)) else 0
            bobot = float(row.get('Bobot', 0)) if not pd.isna(row.get('Bobot', 0)) else None
            jumlah_param = float(row.get('Jumlah_Parameter', 0)) if not pd.isna(row.get('Jumlah_Parameter', 0)) else None
            
            gcg_item = {
                'Tahun': tahun,
                'Skor': skor,
                'Level': level,
                'Section': str(row.get('Section', '')),
                'Capaian': capaian,
                'Bobot': bobot,
                'Jumlah_Parameter': jumlah_param,
                'Penjelasan': str(row.get('Penjelasan', '')),
                'Penilai': str(row.get('Penilai', 'Unknown')),
                'No': str(row.get('No', '')),
                'Deskripsi': str(row.get('Deskripsi', '')),
                'Jenis_Penilaian': str(row.get('Jenis_Penilaian', 'Data Kosong'))
            }
            gcg_data.append(gcg_item)
        
        return jsonify({
            'success': True,
            'data': gcg_data,
            'total_rows': len(gcg_data),
            'available_years': list(set([item['Tahun'] for item in gcg_data])),
            'message': f'Loaded GCG chart data: {len(gcg_data)} rows'
        })
        
    except Exception as e:
        print(f"ERROR: Error loading GCG chart data: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'data': []
        }), 500


@app.route('/api/gcg-mapping', methods=['GET'])
def get_gcg_mapping():
    """
    Get GCG mapping data for autocomplete suggestions
    """
    try:
        # Path to GCG mapping CSV file
        gcg_mapping_path = Path(__file__).parent.parent / 'GCG_MAPPING.csv'
        
        if not gcg_mapping_path.exists():
            print(f"WARNING: GCG_MAPPING.csv not found at: {gcg_mapping_path}")
            return jsonify({
                'success': False,
                'error': 'GCG mapping file not found',
                'data': []
            }), 404
        
        # Read GCG mapping CSV
        df = pd.read_csv(gcg_mapping_path)
        
        # Convert to list of dictionaries for JSON response
        gcg_data = []
        for _, row in df.iterrows():
            gcg_item = {
                'level': str(row.get('Level', '')),
                'type': str(row.get('Type', '')),
                'section': str(row.get('Section', '')),
                'no': str(row.get('No', '')),
                'deskripsi': str(row.get('Deskripsi', '')),
                'jumlah_parameter': str(row.get('Jumlah_Parameter', '')),
                'bobot': str(row.get('Bobot', ''))
            }
            gcg_data.append(gcg_item)
        
        # Return all items for flexible filtering on frontend
        return jsonify({
            'success': True,
            'data': gcg_data,
            'total_items': len(gcg_data),
            'headers': len([item for item in gcg_data if item['type'] == 'header']),
            'indicators': len([item for item in gcg_data if item['type'] == 'indicator']),
            'message': f'Loaded {len(gcg_data)} GCG items for autocomplete'
        })
        
    except Exception as e:
        print(f"ERROR: Error loading GCG mapping: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'data': []
        }), 500

@app.route('/api/cleanup-orphaned-data', methods=['POST'])
def cleanup_orphaned_data():
    """
    Clean up orphaned entries in assessments.json that don't exist in output.xlsx
    """
    try:
        assessments_path = Path(__file__).parent.parent / 'web-output' / 'assessments.json'
        
        # Get years that exist in output.xlsx
        xlsx_years = set()
        df = storage_service.read_excel('web-output/output.xlsx')
        if df is not None:
            xlsx_years = set(df['Tahun'].unique())
            print(f"INFO: Found years in output.xlsx: {sorted(xlsx_years)}")
        else:
            print("WARNING: output.xlsx not found - will clean all assessments.json entries")
        
        # Clean up assessments.json
        orphaned_count = 0
        if assessments_path.exists():
            with open(assessments_path, 'r') as f:
                assessments_data = json.load(f)
            
            original_count = len(assessments_data.get('assessments', []))
            
            # Filter out orphaned entries (keep only years that exist in xlsx or if xlsx doesn't exist, keep none)
            cleaned_assessments = []
            for assessment in assessments_data.get('assessments', []):
                year = assessment.get('year')
                if year in xlsx_years:
                    cleaned_assessments.append(assessment)
                else:
                    orphaned_count += 1
                    print(f"CLEANUP: Removing orphaned assessment for year {year}")
            
            # Save cleaned data
            assessments_data['assessments'] = cleaned_assessments
            with open(assessments_path, 'w') as f:
                json.dump(assessments_data, f, indent=2)
                
            print(f"SUCCESS: Cleaned up {orphaned_count} orphaned entries from assessments.json")
            print(f"INFO: Kept {len(cleaned_assessments)} valid entries")
        else:
            print("WARNING: assessments.json not found - nothing to clean")
        
        return jsonify({
            'success': True,
            'message': f'Successfully cleaned up {orphaned_count} orphaned entries',
            'orphaned_count': orphaned_count,
            'xlsx_years': sorted(list(xlsx_years)),
            'xlsx_exists': storage_service.file_exists('web-output/output.xlsx'),
            'assessments_exists': assessments_path.exists()
        })
        
    except Exception as e:
        print(f"ERROR: Error during cleanup: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'Failed to cleanup orphaned data'
        }), 500


@app.route('/api/uploaded-files', methods=['GET'])
def get_uploaded_files():
    """Get all uploaded files from Supabase storage."""
    try:
        # Get year filter from query parameters
        year = request.args.get('year')
        
        # Read uploaded files data from storage
        files_data = storage_service.read_excel('uploaded-files.xlsx')
        
        if files_data is None:
            # Return empty list if no files exist yet
            return jsonify({'files': []}), 200
        
        # Convert DataFrame to list of dictionaries
        files_list = files_data.to_dict('records')
        
        # Filter by year if provided
        if year:
            try:
                year_int = int(year)
                files_list = [f for f in files_list if f.get('year') == year_int]
            except ValueError:
                return jsonify({'error': 'Invalid year parameter'}), 400
        
        return jsonify({'files': files_list}), 200
        
    except Exception as e:
        print(f"Error getting uploaded files: {e}")
        return jsonify({'error': f'Failed to get uploaded files: {str(e)}'}), 500

@app.route('/api/uploaded-files', methods=['POST'])
def create_uploaded_file():
    """Add a new uploaded file record to Supabase storage."""
    try:
        data = request.get_json()
        
        # Validate required fields
        required_fields = ['fileName', 'fileSize', 'year', 'uploadDate']
        for field in required_fields:
            if field not in data:
                return jsonify({'error': f'Missing required field: {field}'}), 400
        
        # Read existing files data
        try:
            files_data = storage_service.read_excel('uploaded-files.xlsx')
            if files_data is None:
                # Create new DataFrame if no data exists
                files_data = pd.DataFrame()
        except:
            files_data = pd.DataFrame()
        
        # Generate unique ID
        new_id = str(uuid.uuid4())
        
        # Create new file record
        new_file = {
            'id': new_id,
            'fileName': data['fileName'],
            'fileSize': data['fileSize'],
            'uploadDate': data['uploadDate'],
            'year': data['year'],
            'checklistId': data.get('checklistId'),
            'checklistDescription': data.get('checklistDescription'),
            'aspect': data.get('aspect', 'Tidak Diberikan Aspek'),
            'subdirektorat': data.get('subdirektorat'),
            'catatan': data.get('catatan'),
            'status': 'uploaded'
        }
        
        # Add to DataFrame
        new_row = pd.DataFrame([new_file])
        files_data = pd.concat([files_data, new_row], ignore_index=True)
        
        # Save to storage
        success = storage_service.write_excel(files_data, 'uploaded-files.xlsx')
        
        if success:
            return jsonify({'success': True, 'file': new_file}), 201
        else:
            return jsonify({'error': 'Failed to save file to storage'}), 500
            
    except Exception as e:
        print(f"Error creating uploaded file: {e}")
        return jsonify({'error': f'Failed to create uploaded file: {str(e)}'}), 500

@app.route('/api/uploaded-files/<file_id>', methods=['DELETE'])
def delete_uploaded_file(file_id):
    """Delete an uploaded file record from Supabase storage."""
    try:
        # Read existing files data
        files_data = storage_service.read_excel('uploaded-files.xlsx')
        
        if files_data is None:
            return jsonify({'error': 'No files data found'}), 404
        
        # Find and remove the file
        initial_count = len(files_data)
        files_data = files_data[files_data['id'] != file_id]
        
        if len(files_data) == initial_count:
            return jsonify({'error': 'File not found'}), 404
        
        # Save updated data
        success = storage_service.write_excel(files_data, 'uploaded-files.xlsx')
        
        if success:
            return jsonify({'success': True}), 200
        else:
            return jsonify({'error': 'Failed to save changes to storage'}), 500
            
    except Exception as e:
        print(f"Error deleting uploaded file: {e}")
        return jsonify({'error': f'Failed to delete uploaded file: {str(e)}'}), 500

@app.route('/api/upload-gcg-file', methods=['POST'])
def upload_gcg_file():
    """
    Upload a GCG document file directly to Supabase storage.
    This endpoint is specifically for the monitoring upload feature.
    """
    try:
        print(f"ðŸ”§ DEBUG: GCG file upload request received")
        print(f"ðŸ”§ DEBUG: Request files: {list(request.files.keys())}")
        print(f"ðŸ”§ DEBUG: Request form: {dict(request.form)}")
        
        # Check if file is present
        if 'file' not in request.files:
            print(f"ðŸ”§ DEBUG: No file in request")
            return jsonify({'error': 'No file provided'}), 400
        
        file = request.files['file']
        print(f"ðŸ”§ DEBUG: File received: {file.filename}")
        
        if file.filename == '':
            print(f"ðŸ”§ DEBUG: Empty filename")
            return jsonify({'error': 'No file selected'}), 400
        
        # Get metadata from form
        year = request.form.get('year')
        checklist_id = request.form.get('checklistId')
        checklist_description = request.form.get('checklistDescription', '')
        aspect = request.form.get('aspect', '')
        subdirektorat = request.form.get('subdirektorat', '')
        catatan = request.form.get('catatan', '')
        
        # Validate required fields
        if not year:
            return jsonify({'error': 'Year is required'}), 400
        
        try:
            year_int = int(year)
        except ValueError:
            return jsonify({'error': 'Invalid year format'}), 400
        
        # Generate unique file path in Supabase
        file_id = str(uuid.uuid4())
        file_extension = file.filename.rsplit('.', 1)[1].lower() if '.' in file.filename else 'bin'
        supabase_file_path = f"gcg-documents/{year_int}/{file_id}_{secure_filename(file.filename)}"
        
        # Upload file to Supabase storage
        file_data = file.read()
        
        # Determine content type
        content_type_map = {
            'pdf': 'application/pdf',
            'doc': 'application/msword',
            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'xls': 'application/vnd.ms-excel',
            'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            'ppt': 'application/vnd.ms-powerpoint',
            'pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'
        }
        content_type = content_type_map.get(file_extension, 'application/octet-stream')
        
        # Upload to Supabase using storage service
        try:
            from supabase import create_client
            supabase_url = os.getenv('SUPABASE_URL')
            supabase_key = os.getenv('SUPABASE_KEY')
            bucket_name = os.getenv('SUPABASE_BUCKET')
            
            supabase = create_client(supabase_url, supabase_key)
            
            # Try to upload
            response = supabase.storage.from_(bucket_name).upload(
                path=supabase_file_path,
                file=file_data,
                file_options={"content-type": content_type}
            )
            
            # If upload fails due to existing file, try update
            if hasattr(response, 'error') and response.error:
                if "already exists" in str(response.error):
                    print(f"ðŸ”§ DEBUG: File exists, trying update...")
                    response = supabase.storage.from_(bucket_name).update(
                        path=supabase_file_path,
                        file=file_data,
                        file_options={"content-type": content_type}
                    )
                else:
                    print(f"ðŸ”§ DEBUG: Upload error: {response.error}")
                    return jsonify({'error': f'Failed to upload file to storage: {response.error}'}), 500
            
            print(f"ðŸ”§ DEBUG: File uploaded successfully to: {supabase_file_path}")
            
        except Exception as upload_error:
            print(f"ðŸ”§ DEBUG: Upload exception: {upload_error}")
            return jsonify({'error': f'Failed to upload file: {str(upload_error)}'}), 500
        
        # Create file record
        file_record = {
            'id': file_id,
            'fileName': file.filename,
            'fileSize': len(file_data),
            'uploadDate': datetime.now().isoformat(),
            'year': year_int,
            'checklistId': int(checklist_id) if checklist_id else None,
            'checklistDescription': checklist_description,
            'aspect': aspect,
            'subdirektorat': subdirektorat,
            'catatan': catatan,
            'status': 'uploaded',
            'supabaseFilePath': supabase_file_path
        }
        
        # Add to uploaded files database
        try:
            files_data = storage_service.read_excel('uploaded-files.xlsx')
            if files_data is None:
                files_data = pd.DataFrame()
        except:
            files_data = pd.DataFrame()
        
        new_row = pd.DataFrame([file_record])
        files_data = pd.concat([files_data, new_row], ignore_index=True)
        
        # Save to storage
        success = storage_service.write_excel(files_data, 'uploaded-files.xlsx')
        
        if success:
            print(f"ðŸ”§ DEBUG: File record saved successfully")
            return jsonify({
                'success': True, 
                'file': file_record,
                'message': 'File uploaded successfully to Supabase'
            }), 201
        else:
            return jsonify({'error': 'File uploaded but failed to save record'}), 500
        
    except Exception as e:
        print(f"ðŸ”§ DEBUG: Exception in upload_gcg_file: {e}")
        import traceback
        print(f"ðŸ”§ DEBUG: Full traceback: {traceback.format_exc()}")
        return jsonify({'error': f'Failed to upload GCG file: {str(e)}'}), 500


if __name__ == '__main__':
    print(">> Starting POS Data Cleaner 2 Web API")
    print(f"   Upload folder: {UPLOAD_FOLDER}")
    print(f"   Output folder: {OUTPUT_FOLDER}")
    print("   CORS enabled for React frontend")
    print("   Production system integrated")
    print("   Server starting on http://localhost:5000")
    
    app.run(debug=True, host='0.0.0.0', port=5000)